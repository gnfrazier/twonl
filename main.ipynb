{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Walks of Nathan Lowell\n",
    "A look at Nate's path to the back gate.\n",
    "\n",
    "The journey begins [#tommw](https://www.nathanlowell.com/tommw/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archive File Structure\n",
    "\n",
    "``` python\n",
    "{'meta': {'ids':[\n",
    "                '12345',\n",
    "                '67890',\n",
    "                ...]\n",
    "          'last_updated': 'UTC Timestamp',\n",
    "         }\n",
    " 'data': [\n",
    "     {'link':URl string,\n",
    "      'condition': string,\n",
    "      ...}\n",
    "         ]\n",
    "}\n",
    " \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[#tommw](https://www.nathanlowell.com/tommw/) is a cross channel social media campaign architeched and executed by science fiction author Nathan Lowell. \n",
    "\n",
    "There are 4 planks to Nathan's campain:\n",
    "1. A blog hosted at https://www.nathanlowell.com/tommw/\n",
    "2. A photo stream hosted on [FlickR](https://www.flickr.com/photos/nlowell/)\n",
    "3. Podcast feed via [RSS](http://www.nathanlowell.com/tommw/feed/)\n",
    "4. Posts to twitter with the #tommw hastag on [@nlowell on twitter](https://twitter.com/nlowell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive loaded.\n",
      "tweets_loaded.\n",
      "48659243562 added\n",
      "48653036051 added\n",
      "48647365171 added\n",
      "48643230862 added\n",
      "no #tommow tag found\n",
      "48636113586 added\n",
      "48630324366 added\n",
      "48624487397 added\n",
      "48617253282 added\n",
      "48611177168 added\n",
      "48607081976 added\n",
      "48599695671 added\n",
      "48592055857 added\n",
      "48584257987 added\n",
      "48575278411 added\n",
      "48566453132 added\n",
      "48558267012 added\n",
      "48550889086 added\n",
      "48543445341 added\n",
      "48535977336 added\n",
      "48528019971 added\n",
      "48519612757 added\n",
      "48510717431 added\n",
      "48502988887 added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['#tommw since:2019-07-01 until:2019-07-03', '#tommw since:2019-07-03 until:2019-07-05', '#tommw since:2019-07-05 until:2019-07-07', '#tommw since:2019-07-07 until:2019-07-09', '#tommw since:2019-07-09 until:2019-07-11', '#tommw since:2019-07-11 until:2019-07-13', '#tommw since:2019-07-13 until:2019-07-15', '#tommw since:2019-07-15 until:2019-07-18', '#tommw since:2019-07-18 until:2019-07-20', '#tommw since:2019-07-20 until:2019-07-22', '#tommw since:2019-07-22 until:2019-07-24', '#tommw since:2019-07-24 until:2019-07-26', '#tommw since:2019-07-26 until:2019-07-28', '#tommw since:2019-07-28 until:2019-07-30', '#tommw since:2019-07-30 until:2019-08-02', '#tommw since:2019-08-02 until:2019-08-04', '#tommw since:2019-08-04 until:2019-08-06', '#tommw since:2019-08-06 until:2019-08-08', '#tommw since:2019-08-08 until:2019-08-10', '#tommw since:2019-08-10 until:2019-08-12', '#tommw since:2019-08-12 until:2019-08-14', '#tommw since:2019-08-14 until:2019-08-16', '#tommw since:2019-08-16 until:2019-08-19', '#tommw since:2019-08-19 until:2019-08-21', '#tommw since:2019-08-21 until:2019-08-23', '#tommw since:2019-08-23 until:2019-08-25', '#tommw since:2019-08-25 until:2019-08-27', '#tommw since:2019-08-27 until:2019-08-29', '#tommw since:2019-08-29 until:2019-08-31', '#tommw since:2019-08-31 until:2019-09-03']\n",
      "INFO: Querying #tommw since:2019-08-21 until:2019-08-23\n",
      "INFO: Querying #tommw since:2019-08-31 until:2019-09-03\n",
      "INFO: Querying #tommw since:2019-08-19 until:2019-08-21\n",
      "INFO: Querying #tommw since:2019-07-01 until:2019-07-03\n",
      "INFO: Querying #tommw since:2019-07-03 until:2019-07-05\n",
      "INFO: Querying #tommw since:2019-07-18 until:2019-07-20\n",
      "INFO: Querying #tommw since:2019-07-05 until:2019-07-07\n",
      "INFO: Querying #tommw since:2019-08-16 until:2019-08-19\n",
      "INFO: Querying #tommw since:2019-08-10 until:2019-08-12\n",
      "INFO: Querying #tommw since:2019-07-28 until:2019-07-30\n",
      "INFO: Querying #tommw since:2019-07-22 until:2019-07-24\n",
      "INFO: Querying #tommw since:2019-08-29 until:2019-08-31\n",
      "INFO: Querying #tommw since:2019-07-11 until:2019-07-13\n",
      "INFO: Querying #tommw since:2019-07-13 until:2019-07-15\n",
      "INFO: Querying #tommw since:2019-07-09 until:2019-07-11\n",
      "INFO: Querying #tommw since:2019-07-15 until:2019-07-18\n",
      "INFO: Querying #tommw since:2019-08-23 until:2019-08-25\n",
      "INFO: Querying #tommw since:2019-07-26 until:2019-07-28\n",
      "INFO: Querying #tommw since:2019-07-24 until:2019-07-26\n",
      "INFO: Querying #tommw since:2019-07-07 until:2019-07-09\n",
      "INFO: Querying #tommw since:2019-08-14 until:2019-08-16\n",
      "INFO: Querying #tommw since:2019-07-20 until:2019-07-22\n",
      "INFO: Querying #tommw since:2019-08-12 until:2019-08-14\n",
      "INFO: Querying #tommw since:2019-08-27 until:2019-08-29\n",
      "INFO: Querying #tommw since:2019-08-25 until:2019-08-27\n",
      "INFO: Querying #tommw since:2019-08-06 until:2019-08-08\n",
      "INFO: Querying #tommw since:2019-08-02 until:2019-08-04\n",
      "INFO: Querying #tommw since:2019-08-08 until:2019-08-10\n",
      "INFO: Querying #tommw since:2019-07-30 until:2019-08-02\n",
      "INFO: Querying #tommw since:2019-08-04 until:2019-08-06\n",
      "INFO: Got 0 tweets for %23tommw%20since%3A2019-07-07%20until%3A2019-07-09.\n",
      "INFO: Got 0 tweets (0 new).\n",
      "INFO: Got 2 tweets for %23tommw%20since%3A2019-07-03%20until%3A2019-07-05.\n",
      "INFO: Got 2 tweets (2 new).\n",
      "INFO: Got 2 tweets for %23tommw%20since%3A2019-07-22%20until%3A2019-07-24.\n",
      "INFO: Got 4 tweets (2 new).\n",
      "INFO: Got 2 tweets for %23tommw%20since%3A2019-07-13%20until%3A2019-07-15.\n",
      "INFO: Got 6 tweets (2 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-08-10%20until%3A2019-08-12.\n",
      "INFO: Got 10 tweets (4 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-08-06%20until%3A2019-08-08.\n",
      "INFO: Got 14 tweets (4 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-08-23%20until%3A2019-08-25.\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-07-28%20until%3A2019-07-30.\n",
      "INFO: Got 18 tweets (4 new).\n",
      "INFO: Got 22 tweets (4 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-07-01%20until%3A2019-07-03.\n",
      "INFO: Got 26 tweets (4 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-08-19%20until%3A2019-08-21.\n",
      "INFO: Got 30 tweets (4 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-08-08%20until%3A2019-08-10.\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-08-14%20until%3A2019-08-16.\n",
      "INFO: Got 34 tweets (4 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-08-04%20until%3A2019-08-06.\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-07-09%20until%3A2019-07-11.\n",
      "INFO: Got 38 tweets (4 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-07-18%20until%3A2019-07-20.\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-07-24%20until%3A2019-07-26.\n",
      "INFO: Got 42 tweets (4 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-08-31%20until%3A2019-09-03.\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-08-21%20until%3A2019-08-23.\n",
      "INFO: Got 46 tweets (4 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-08-12%20until%3A2019-08-14.\n",
      "INFO: Got 50 tweets (4 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-08-25%20until%3A2019-08-27.\n",
      "INFO: Got 54 tweets (4 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-08-27%20until%3A2019-08-29.\n",
      "INFO: Got 58 tweets (4 new).\n",
      "INFO: Got 62 tweets (4 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-08-29%20until%3A2019-08-31.\n",
      "INFO: Got 66 tweets (4 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-07-20%20until%3A2019-07-22.\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-07-11%20until%3A2019-07-13.\n",
      "INFO: Got 70 tweets (4 new).\n",
      "INFO: Got 74 tweets (4 new).\n",
      "INFO: Got 78 tweets (4 new).\n",
      "INFO: Got 82 tweets (4 new).\n",
      "INFO: Got 86 tweets (4 new).\n",
      "INFO: Got 6 tweets for %23tommw%20since%3A2019-07-30%20until%3A2019-08-02.\n",
      "INFO: Got 92 tweets (6 new).\n",
      "INFO: Got 4 tweets for %23tommw%20since%3A2019-07-26%20until%3A2019-07-28.\n",
      "INFO: Got 96 tweets (4 new).\n",
      "INFO: Got 6 tweets for %23tommw%20since%3A2019-07-05%20until%3A2019-07-07.\n",
      "INFO: Got 102 tweets (6 new).\n",
      "INFO: Got 6 tweets for %23tommw%20since%3A2019-08-02%20until%3A2019-08-04.\n",
      "INFO: Got 108 tweets (6 new).\n",
      "INFO: Got 6 tweets for %23tommw%20since%3A2019-07-15%20until%3A2019-07-18.\n",
      "INFO: Got 114 tweets (6 new).\n",
      "INFO: Got 8 tweets for %23tommw%20since%3A2019-08-16%20until%3A2019-08-19.\n",
      "INFO: Got 122 tweets (8 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to parse title\n",
      "Unable to parse title\n",
      "Unable to parse title\n",
      "Unable to parse title\n",
      "116  walks added to archive.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "\n",
    "import arrow\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import photo as pto\n",
    "import podcast as pc\n",
    "import twitter as twit\n",
    "\n",
    "# Functions to manage the archive file\n",
    "\n",
    "\n",
    "def get_archive():\n",
    "    '''Loads archive.json, or creates a new blank files structure.'''\n",
    "\n",
    "    # Load the archive file\n",
    "    try:\n",
    "        with open('archive.json', 'r') as file:\n",
    "            archive = json.loads(file.read())\n",
    "        print('Archive loaded.')\n",
    "\n",
    "    except:\n",
    "        # Create archive data structure.\n",
    "\n",
    "        archive = {'meta': {'ids': [],\n",
    "                            'audio': [],\n",
    "                            'tw_ids': [],\n",
    "                            'last_updated': 'New File',\n",
    "                            },\n",
    "                   'data': [],\n",
    "                   }\n",
    "        print('No archived file, building new archive.json')\n",
    "\n",
    "    return archive\n",
    "\n",
    "\n",
    "def save_archive(archive):\n",
    "    '''Saves to archive.json.'''\n",
    "\n",
    "    with open('archive.json', 'w') as file:\n",
    "        archive['meta']['last_updated'] = str(arrow.now())\n",
    "        file.write(json.dumps(archive, indent=4))\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_tweet_archive(record='tweets'):\n",
    "    '''Utility function to load the tweet archvive'''\n",
    "\n",
    "    filename = record + '.json'\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            archive = json.loads(file.read())\n",
    "        print('tweets_loaded.')\n",
    "\n",
    "    except:\n",
    "\n",
    "        print('No archived tweet file: ', filename,\n",
    "              ' build a new file with build_twitter_archive function')\n",
    "\n",
    "    return archive\n",
    "\n",
    "\n",
    "def backfill_dates(archive):\n",
    "    '''Helper function to convert timestamps\n",
    "    to a date. Date is used to match audio files to photos.'''\n",
    "\n",
    "    for walk in archive['data']:\n",
    "\n",
    "        timestamp = walk['timestamp']\n",
    "\n",
    "        date = timestamp.split(' ')[0]\n",
    "\n",
    "        walk['date'] = date\n",
    "\n",
    "    return archive\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Path to temp directory\n",
    "    latest = 'latest/'\n",
    "\n",
    "    # Import the archive from disk\n",
    "    archive = get_archive()\n",
    "    walks = get_tweet_archive('walk')\n",
    "\n",
    "    # Scrape the blog for links to the other media components\n",
    "    blog_page = requests.get(\"https://www.nathanlowell.com/tommw/\")\n",
    "    soup = BeautifulSoup(blog_page.text, 'html.parser')\n",
    "\n",
    "    # Find a flickr link\n",
    "    flickr = soup.find_all(href=True, attrs={'data-flickr-embed': 'true'})\n",
    "\n",
    "    # Find a podcast link\n",
    "    try:\n",
    "        podcast_link = soup.find(class_=\"powerpress_link_pinw\").get('href')\n",
    "\n",
    "    except:\n",
    "\n",
    "        print('No new walk posts found.')\n",
    "        podcast_link = None\n",
    "\n",
    "    if not flickr:\n",
    "        print('No flickr links found on blog home page.')\n",
    "        flickr_link = None\n",
    "\n",
    "    else:\n",
    "        flickr_link = flickr[0]['href']\n",
    "\n",
    "    if not podcast_link and not flickr_link:\n",
    "        print('No recent walks found on blog.')\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Process the photostream\n",
    "        stream_url = pto.get_stream_url(flickr_link)\n",
    "        stream = pto.get_photostream(stream_url)\n",
    "        archive = pto.process_photo_stream_page(stream, archive)\n",
    "\n",
    "        # Save the audio file\n",
    "        mp3_file, archive = pc.get_mp3(podcast_link, archive)\n",
    "\n",
    "        if len(archive['meta']['ids']) < 400:\n",
    "            album_url = 'https://www.flickr.com/photos/' +\\\n",
    "                'nlowell/albums/72157626736309035'\n",
    "            archive = pto.build_flickr_archive(\n",
    "                flickr_link, archive, album=False)\n",
    "            archive = pto.build_flickr_archive(album_url, archive, album=True)\n",
    "\n",
    "    # Check the album page\n",
    "    album_url = 'https://www.flickr.com/photos/' +\\\n",
    "        'nlowell/albums/72157626736309035'\n",
    "    stream = pto.get_photostream(album_url, album=True)\n",
    "    archive = pto.process_photo_stream_page(stream, archive)\n",
    "\n",
    "    # Check twitter\n",
    "    \n",
    "    tweets = twit.new_tweets(archive, start_date=None)\n",
    "    filtered = twit.filter_twitter_search(tweets)\n",
    "    walks = filtered['walks']\n",
    "\n",
    "    if walks:\n",
    "        archive = twit.merge_tw_walks_into_photo_walks(archive, walks)\n",
    "        print(len(walks), ' walks added to archive.')\n",
    "\n",
    "    else:\n",
    "        print('No walks added to archive.')\n",
    "\n",
    "    # Save the data to archive.json\n",
    "    save_archive(archive)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
