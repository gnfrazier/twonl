{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Walks of Nathan Lowell\n",
    "A look at Nate's path to the back gate.\n",
    "\n",
    "The journey begins [#tommw](https://www.nathanlowell.com/tommw/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archive File Structure\n",
    "\n",
    "``` python\n",
    "{'meta': {'ids':[\n",
    "                '12345',\n",
    "                '67890',\n",
    "                ...]\n",
    "          'last_updated': 'UTC Timestamp',\n",
    "         }\n",
    " 'data': [\n",
    "     {'link':URl string,\n",
    "      'condition': string,\n",
    "      ...}\n",
    "         ]\n",
    "}\n",
    " \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import json\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import arrow\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import photo as pto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[#tommw](https://www.nathanlowell.com/tommw/) is a cross channel social media campaign architeched and executed by science fiction author Nathan Lowell. \n",
    "\n",
    "There are 4 planks to Nathan's campain:\n",
    "1. A blog hosted at https://www.nathanlowell.com/tommw/\n",
    "2. A photo stream hosted on [FlickR](https://www.flickr.com/photos/nlowell/)\n",
    "3. Podcast feed via [RSS](http://www.nathanlowell.com/tommw/feed/)\n",
    "4. Posts to twitter with the #tommw hastag on [@nlowell on twitter](https://twitter.com/nlowell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to manage the archive file\n",
    "\n",
    "def get_archive():\n",
    "\n",
    "    # Load the archive file\n",
    "    try:\n",
    "        with open('archive.json', 'r') as file:\n",
    "            archive= json.loads(file.read())\n",
    "        print('Archive loaded.')\n",
    "        \n",
    "    except:\n",
    "        # Create archive data structure.\n",
    "\n",
    "        archive = {'meta':{'ids':[],\n",
    "                           'audio':[],\n",
    "                          'last_updated':'New File',\n",
    "                          },\n",
    "                  'data':[],\n",
    "                  }\n",
    "        print('No archived file, building new archive.json')\n",
    "        \n",
    "    return archive\n",
    "\n",
    "def save_archive(archive):\n",
    "    \n",
    "    with open('archive.json', 'w') as file:\n",
    "        archive['meta']['last_updated'] = str(arrow.now())\n",
    "        file.write(json.dumps(archive, indent=4))\n",
    "        \n",
    "    return True\n",
    "\n",
    "def backfill_dates(archive):\n",
    "    '''Helper function to convert timestamps\n",
    "    to a date. Date is used to match audio files to photos.'''\n",
    "\n",
    "    for walk in archive['data']:\n",
    "\n",
    "        timestamp = walk['timestamp']\n",
    "\n",
    "        date = timestamp.split(' ')[0]\n",
    "\n",
    "        walk['date'] = date\n",
    "\n",
    "    return archive\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future podcast.py\n",
    "\n",
    "import requests\n",
    "import tinytag\n",
    "\n",
    "\n",
    "def get_mp3(podcast_link, archive):\n",
    "\n",
    "    mp3_name = podcast_link.split('/')[-1]\n",
    "\n",
    "    mp3_file = 'audio/' + mp3_name\n",
    "\n",
    "    if mp3_name in archive['meta']['audio']:\n",
    "        # skip files already downloaded\n",
    "        return mp3_file, archive\n",
    "\n",
    "    file = requests.get(podcast_link)\n",
    "\n",
    "    if file.status_code != 200:\n",
    "\n",
    "        return False\n",
    "\n",
    "    with open(mp3_file, 'wb') as f:\n",
    "        f.write(file.content)\n",
    "\n",
    "    archive['meta']['audio'].append(mp3_name)\n",
    "\n",
    "    return mp3_file, archive\n",
    "\n",
    "\n",
    "\n",
    "def get_id3_tag(mp3_file):\n",
    "\n",
    "    id_info = tinytag.TinyTag.get(mp3_file).as_dict()\n",
    "\n",
    "    title = id_info.get('title')\n",
    "\n",
    "    if title:\n",
    "        date = title.split(' ')[-1]\n",
    "    else:\n",
    "        date = 'unknown'\n",
    "\n",
    "    id_info['date'] = date\n",
    "\n",
    "    id_info['audio'] = mp3_file\n",
    "    \n",
    "\n",
    "    return id_info\n",
    "\n",
    "\n",
    "def match_photo_date(id_info, archive):\n",
    "    '''Iterates through the archive to match audio\n",
    "    to a photo record'''\n",
    "\n",
    "    for walk in archive['data']:\n",
    "\n",
    "        if walk['date'] == id_info['date']:\n",
    "\n",
    "            walk.update(id_info)\n",
    "            mp3_file = id_info.get('audio')\n",
    "            \n",
    "            if mp3_file.find('.mp3') > 1:\n",
    "                mp3_name = mp3_file.strip('audio/')\n",
    "                archive['meta']['audio'].append(mp3_name)\n",
    "                \n",
    "            else:\n",
    "                print('Unable to get mp3 info from', id_info)\n",
    "\n",
    "    return archive\n",
    "\n",
    "\n",
    "def build_audio_archive(podcast_link, archive):\n",
    "    # TODO complete this function\n",
    "    # Parse URL into base, date, file_name\n",
    "    podcast_link.rsplit('/', maxsplit=2)\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive loaded.\n",
      "no #tommow tag found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Future Main\n",
    "\n",
    "# Path to temp directory\n",
    "latest = 'latest/'\n",
    "\n",
    "# Import the archive from disk\n",
    "archive = get_archive()\n",
    "\n",
    "# Scrape the blog for links to the other media components\n",
    "blog_page = requests.get(\"https://www.nathanlowell.com/tommw/\")\n",
    "soup = BeautifulSoup(blog_page.text, 'html.parser')\n",
    "\n",
    "# Find a flickr link\n",
    "flickr = soup.find_all(href=True, attrs={'data-flickr-embed':'true'})\n",
    "\n",
    "# Find a podcast link\n",
    "podcast_link = soup.find(class_=\"powerpress_link_pinw\").get('href')\n",
    "\n",
    "if not flickr:\n",
    "    print('No flickr links found on blog home page.')\n",
    "    flicker_link = None\n",
    "    \n",
    "else:\n",
    "    flickr_link = flickr[0]['href']\n",
    "\n",
    "    \n",
    "if len(archive['meta']['ids']) < 400:\n",
    "    archive = pto.build_flickr_archive(flickr_link, archive)\n",
    "    \n",
    "\n",
    "# Process the photostream\n",
    "stream_url = pto.get_stream_url(flickr_link)\n",
    "stream = pto.get_photostream(stream_url)\n",
    "archive = pto.process_photo_stream_page(stream, archive)\n",
    "\n",
    "# Check the album page\n",
    "album_url = 'https://www.flickr.com/photos/nlowell/albums/72157626736309035'\n",
    "stream = pto.get_photostream(album_url, album=True)\n",
    "archive = pto.process_photo_stream_page(stream, archive)\n",
    "\n",
    "\n",
    "# Save the audio file\n",
    "mp3_file, archive = get_mp3(podcast_link, archive)\n",
    "\n",
    "# Save the data to archive.json\n",
    "save_archive(archive)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.flickr.com/photos/nlowell/albums/72157626736309035/page1\n",
      "https://www.flickr.com/photos/nlowell/albums/72157626736309035/page2\n",
      "https://www.flickr.com/photos/nlowell/albums/72157626736309035/page3\n",
      "no #tommow tag found\n",
      "https://www.flickr.com/photos/nlowell/albums/72157626736309035/page4\n",
      "no #tommow tag found\n",
      "no #tommow tag found\n",
      "no #tommow tag found\n",
      "https://www.flickr.com/photos/nlowell/albums/72157626736309035/page5\n",
      "https://www.flickr.com/photos/nlowell/albums/72157626736309035/page6\n"
     ]
    }
   ],
   "source": [
    "archive = pto.build_flickr_archive(album_url, archive, album=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_archive(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check for blocks of missing photos\n",
    "\n",
    "times = []\n",
    "for photo in archive['data']:\n",
    "    times.append(photo['date'])\n",
    "\n",
    "times.sort()\n",
    "times\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3_file = 'audio/tommw-2018-12-21.mp3'\n",
    "latest = 'latest/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3_file.find('.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wv\n",
    "import pydub\n",
    "import tinytag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3_audio = pydub.AudioSegment.from_mp3(mp3_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to wav\n",
    "mp3_audio.export(latest+\"unprocessed.wav\", format=\"wav\")\n",
    "\n",
    "#read wav file\n",
    "rate,audData=wv.read(latest+\"unprocessed.wav\")\n",
    "\n",
    "# Calculate track time\n",
    "track_time = (audData.shape[0] / rate)/60\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for walk in archive['data']:\n",
    "    print(walk['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = backfill_dates(archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future Twitter.py\n",
    "import toml\n",
    "import twython as tw\n",
    "\n",
    "cred = toml.load('credentials.toml')\n",
    "\n",
    "mm = {'Jan':'01',\n",
    "     'Feb':'02',\n",
    "     'Mar':'03',\n",
    "     'Apr':'04',\n",
    "     'May':'05',\n",
    "     'Jun':'06',\n",
    "     'Jul':'07',\n",
    "     'Aug':'08',\n",
    "     'Sep':'09',\n",
    "     'Oct':'10',\n",
    "     'Nov':'11',\n",
    "     'Dec':'12'}\n",
    "\n",
    "def get_new_twitter_token(cred):\n",
    "    tw_creds = cred['api']['twitter']\n",
    "\n",
    "    twitter = tw.Twython(tw_creds['consumer_key'],\n",
    "                              tw_creds['consumer_secret'],\n",
    "                              oauth_version=2)\n",
    "\n",
    "    ACCESS_TOKEN = twitter.obtain_access_token()\n",
    "    \n",
    "    cred['api']['twitter']['access_token'] = ACCESS_TOKEN\n",
    "    \n",
    "    with open('credentials.toml', 'w') as f:\n",
    "\n",
    "        toml.dump(cred, f)\n",
    "\n",
    "    return ACCESS_TOKEN\n",
    "\n",
    "def get_token(cred):\n",
    "\n",
    "    token = cred['api']['twitter'].get('access_token')\n",
    "\n",
    "\n",
    "    if token == None:\n",
    "        token = get_new_twitter_token(cred)\n",
    "        \n",
    "    return token\n",
    "\n",
    "def get_twitter(token):\n",
    "    \n",
    "    twitter = tw.Twython(tw_creds['consumer_key'],\n",
    "                         access_token=token)\n",
    "\n",
    "def format_tweet_time(tweet_time):\n",
    "    '''Transforms tweet date time\n",
    "    format \"5:11 AM - 16 Jun 2011\"\n",
    "    to timestamp \"2011-06-16 05:11:00\" '''\n",
    "\n",
    "    dt_parts = tweet_time.split(' ')\n",
    "    year = dt_parts[-1]\n",
    "    \n",
    "    # Convert text month to digits\n",
    "    month = mm[dt_parts[-2]]\n",
    "    \n",
    "    day = dt_parts[-3]\n",
    "    raw_time = dt_parts[0].split(':')\n",
    "    hour = raw_time[0].rjust(2,'0')\n",
    "    minute = raw_time[-1]\n",
    "    second = '00'\n",
    "\n",
    "    # Adjust time for am vs pm\n",
    "    if dt_parts[1]=='PM':\n",
    "        hour_num = int(hour) + 12\n",
    "        hour = str(hour)\n",
    "\n",
    "\n",
    "    tw_date = '-'.join([year,month,day])\n",
    "    tw_time = ':'.join([hour,minute,second])\n",
    "    tw_timestamp = ' '.join([tw_date, tw_time])\n",
    "\n",
    "    return tw_timestamp, tw_date, tw_time\n",
    "\n",
    "def process_tweet_body(body):\n",
    "\n",
    "    info = {}\n",
    "    \n",
    "    raw_title = body.split('\"')[1]\n",
    "\n",
    "    comma = raw_title.find(',')\n",
    "\n",
    "    # Find a period with a space after it.\n",
    "    period = raw_title.find('. ')\n",
    "\n",
    "    if comma > 0:\n",
    "        tag_temp = raw_title[:comma]\n",
    "        wind_link = raw_title[comma:]\n",
    "\n",
    "    elif comma < 0 and period > 0:\n",
    "\n",
    "        tag_temp = raw_title[:period]\n",
    "        wind_link = raw_title[period:]\n",
    "\n",
    "    else:\n",
    "        print('Unable to parse title')\n",
    "        info['tag'] = 'Error'\n",
    "        return info\n",
    "\n",
    "    # TODO refactor using deque to improve performance\n",
    "    \n",
    "    t_parts = tag_temp.split(' ')\n",
    "    info['tag'] = t_parts.pop(0)\n",
    "    info['temp'] = t_parts.pop(0)\n",
    "    info['sun'] = ' '.join(t_parts)\n",
    "\n",
    "    w_parts = wind_link.split(' ')\n",
    "    info['photo_link'] = w_parts.pop()\n",
    "    info['wind'] = ' '.join(w_parts)\n",
    "    \n",
    "    return info\n",
    "\n",
    "def process_tweet(tweet_url):\n",
    "    \n",
    "    tweet = requests.get(tweet_url)\n",
    "    tsoup = BeautifulSoup(tweet.text, 'html.parser')\n",
    "    \n",
    "    body = tsoup.title.text\n",
    "    info = process_tweet_body(body)\n",
    "    \n",
    "    tweet_time = str(tsoup.find(class_=\"client-and-actions\").text.strip())\n",
    "    tw_time = format_tweet_time(tweet_time)\n",
    "    \n",
    "    info['tweet_timestamp'], info['date'], info['time'] = tw_time\n",
    "    \n",
    "    info['tweet_id'] = tweet_url.split('/')[-1]\n",
    "    \n",
    "    return info\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    token = get_token(cred)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = 'https://twitter.com/search?l=en&q=%23tommw%20from%3Anlowell%20since%3A2011-06-01%20until%3A2011-07-31&src=typd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_url = 'https://twitter.com/nlowell/status/81333156376096768'\n",
    "tw_info = process_tweet(tweet_url)\n",
    "tw_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = get_archive()\n",
    "archive['data'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive['meta']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
