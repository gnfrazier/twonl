{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Walks of Nathan Lowell\n",
    "A look at Nate's path to the back gate.\n",
    "\n",
    "The journey begins [#tommw](https://www.nathanlowell.com/tommw/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archive File Structure\n",
    "\n",
    "``` python\n",
    "{'meta': {'ids':[\n",
    "                '12345',\n",
    "                '67890',\n",
    "                ...]\n",
    "          'last_updated': 'UTC Timestamp',\n",
    "         }\n",
    " 'data': [\n",
    "     {'link':URl string,\n",
    "      'condition': string,\n",
    "      ...}\n",
    "         ]\n",
    "}\n",
    " \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[#tommw](https://www.nathanlowell.com/tommw/) is a cross channel social media campaign architeched and executed by science fiction author Nathan Lowell. \n",
    "\n",
    "There are 4 planks to Nathan's campain:\n",
    "1. A blog hosted at https://www.nathanlowell.com/tommw/\n",
    "2. A photo stream hosted on [FlickR](https://www.flickr.com/photos/nlowell/)\n",
    "3. Podcast feed via [RSS](http://www.nathanlowell.com/tommw/feed/)\n",
    "4. Posts to twitter with the #tommw hastag on [@nlowell on twitter](https://twitter.com/nlowell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive loaded.\n",
      "tweets_loaded.\n",
      "No new walk posts found.\n",
      "No flickr links found on blog home page.\n",
      "No recent walks found on blog.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['#tommw since:2019-02-12 until:2019-02-13']\n",
      "INFO: Querying #tommw since:2019-02-12 until:2019-02-13\n",
      "INFO: Got 0 tweets for %23tommw%20since%3A2019-02-12%20until%3A2019-02-13.\n",
      "INFO: Got 0 tweets (0 new).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No walks added to archive.\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import json\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import arrow\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "import photo as pto\n",
    "import podcast as pc\n",
    "import twitter as twit\n",
    "\n",
    "# Functions to manage the archive file\n",
    "\n",
    "def get_archive():\n",
    "\n",
    "    # Load the archive file\n",
    "    try:\n",
    "        with open('archive.json', 'r') as file:\n",
    "            archive= json.loads(file.read())\n",
    "        print('Archive loaded.')\n",
    "        \n",
    "    except:\n",
    "        # Create archive data structure.\n",
    "\n",
    "        archive = {'meta':{'ids':[],\n",
    "                           'audio':[],\n",
    "                           'tw_ids':[],\n",
    "                          'last_updated':'New File',\n",
    "                          },\n",
    "                  'data':[],\n",
    "                  }\n",
    "        print('No archived file, building new archive.json')\n",
    "        \n",
    "    return archive\n",
    "\n",
    "def save_archive(archive):\n",
    "    \n",
    "    with open('archive.json', 'w') as file:\n",
    "        archive['meta']['last_updated'] = str(arrow.now())\n",
    "        file.write(json.dumps(archive, indent=4))\n",
    "        \n",
    "    return True\n",
    "\n",
    "\n",
    "def get_tweet_archive(record='tweets'):\n",
    "    \n",
    "    filename = record + '.json'\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            archive= json.loads(file.read())\n",
    "        print('tweets_loaded.')\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print('No archived tweet file: ', filename,\n",
    "              ' build a new file with build_twitter_archive function')\n",
    "        \n",
    "    return archive\n",
    "\n",
    "\n",
    "\n",
    "def backfill_dates(archive):\n",
    "    '''Helper function to convert timestamps\n",
    "    to a date. Date is used to match audio files to photos.'''\n",
    "\n",
    "    for walk in archive['data']:\n",
    "\n",
    "        timestamp = walk['timestamp']\n",
    "\n",
    "        date = timestamp.split(' ')[0]\n",
    "\n",
    "        walk['date'] = date\n",
    "\n",
    "    return archive\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    # Path to temp directory\n",
    "    latest = 'latest/'\n",
    "\n",
    "    # Import the archive from disk\n",
    "    archive = get_archive()\n",
    "    walks = get_tweet_archive('walk')\n",
    "\n",
    "    # Scrape the blog for links to the other media components\n",
    "    blog_page = requests.get(\"https://www.nathanlowell.com/tommw/\")\n",
    "    soup = BeautifulSoup(blog_page.text, 'html.parser')\n",
    "\n",
    "    # Find a flickr link\n",
    "    flickr = soup.find_all(href=True, attrs={'data-flickr-embed':'true'})\n",
    "\n",
    "    # Find a podcast link\n",
    "    try:\n",
    "        podcast_link = soup.find(class_=\"powerpress_link_pinw\").get('href')\n",
    "\n",
    "    except:\n",
    "        print('No new walk posts found.')\n",
    "        podcast_link = None\n",
    "\n",
    "    if not flickr:\n",
    "        print('No flickr links found on blog home page.')\n",
    "        flickr_link = None\n",
    "\n",
    "    else:\n",
    "        flickr_link = flickr[0]['href']\n",
    "\n",
    "\n",
    "    if not podcast_link and not flickr_link:\n",
    "        print('No recent walks found on blog.')\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Process the photostream\n",
    "        stream_url = pto.get_stream_url(flickr_link)\n",
    "        stream = pto.get_photostream(stream_url)\n",
    "        archive = pto.process_photo_stream_page(stream, archive)\n",
    "\n",
    "        # Save the audio file\n",
    "        mp3_file, archive = pc.get_mp3(podcast_link, archive)\n",
    "\n",
    "\n",
    "        if len(archive['meta']['ids']) < 400:\n",
    "            album_url = 'https://www.flickr.com/photos/nlowell/albums/72157626736309035'\n",
    "            archive = pto.build_flickr_archive(flickr_link, archive, album=False)\n",
    "            archive = pto.build_flickr_archive(album_url, archive, album=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Check the album page\n",
    "    album_url = 'https://www.flickr.com/photos/nlowell/albums/72157626736309035'\n",
    "    stream = pto.get_photostream(album_url, album=True)\n",
    "    archive = pto.process_photo_stream_page(stream, archive)\n",
    "\n",
    "\n",
    "\n",
    "    # Check twitter\n",
    "\n",
    "    tweets = twit.new_tweets(archive)\n",
    "    filtered = twit.filter_twitter_search(tweets)\n",
    "    walks = filtered['walks']\n",
    "\n",
    "    if walks:\n",
    "        archive = twit.merge_tw_walks_into_photo_walks(archive, walks)\n",
    "        print(len(walks) + ' walks added to archive.')\n",
    "\n",
    "    else:\n",
    "        print('No walks added to archive.')\n",
    "\n",
    "    # Save the data to archive.json\n",
    "    save_archive(archive)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
